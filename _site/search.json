[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "A2",
    "section": "",
    "text": "Our A2 explores how machine learning can recognize five classical mudras using hand gesture detection, and play a specific song for each one.\n:::: &lt;/p&gt;\n\n:::\nDownload Train ZIP file\nDownload Test ZIP file\nProcess\nWe started by collecting images for each mudra. It was honestly tricky because some friends found it hard to pose like, they literally couldn’t bend their fingers that way.\nWhile researching how to train a model using images, we first came across TensorFlow. Then we found ml5.js was built on top of it. Arvind told us TensorFlow might be too complicated for us right now, so we went ahead with ml5.js.\nBefore jumping into handpose, we also tried out Feature Extractor with MobileNet. Feature Extractor uses MobileNet, which is a pre-trained ML model. After some trial and error and getting lost a bit, we sat down and made a proper step-by-step list to get clarity:\n\n\nUse Python to generate a CSV with image paths and labels.\nUse a JS sketch to extract 21 keypoints for each image and generate a new CSV.\nTrain the model using this second CSV.\nTest the model with webcam input.\nAdd a song trigger for each mudra.\n\nHandpose detection on a single image worked perfectly we were able to extract 21 points and got excited that we could download a CSV and it was actually working.\nCreating the first CSV using Python was pretty simple.\nThe challenge started when we wrote a JS sketch to read that CSV and output the 21 keypoints for all the images. Initially, it was only giving us values for one image, or just the headings (x1, y1… x21, y21, label) but no actual values. We figured this was because the function to read all rows was in preload(), which runs too early the model takes time to train and wasn’t ready yet, so it only had time to process the first row. Once we moved that part to setup(), everything worked fine and we got the full CSV with coordinates. Another small mistake was that we were usign an older ml5js version and that was causing a few errors.\nTraining\nA few images just wouldn’t get detected, especially ones where the hands had paint or black charcoal. We had to delete those.\nAll our training and testing so far was with our own dataset and images. So the next step was to combine other teams’ CSVs and train the model together with all of them, then link the mudras to songs.\nWhen we trained just using our team’s images, everything worked fine. But once we started combining datasets, label issues came up.\nFor example, “Arala” vs “arala” were treated as different classes, same with “katakamukha” vs “kataka-mukha”. While combining, we also saw that the pixel value ranges were totally different. One group had values ranging from 500 to 1000, while ours was more like 160 to 300. That’s when we realized we’d need to normalize the values before training.\nTrying varying epochs: 100 to 150 was enough for the samples that we have. We tried to understand the loss graph and we used it like a guide to find the sweet spot between the underfitting and overfitting. Epochs and batch size - It happens automatically if we don’t specify, and we didn’t do that because ml5 internally passes control to TensorFlow.js which sets a default batch size of 32.\n\nOnce we sorted all of that, the last step was adding the songs."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "InkaOkati",
    "section": "",
    "text": "A1\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\nApr 25, 2025\n\n\nSriharshitha Bhagvati\n\n\n\n\n\n\n\n\n\n\n\n\nA2\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\nApr 25, 2025\n\n\nAadil Tanwir, Naman Rajoria, Rayna Singh, Soumya Saboo, Sriharshitha Bhagvati\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "A1",
    "section": "",
    "text": "This interactive p5.js project brings a lively spring scene to life with animated bees, falling petals, fractal cherry trees, spiral flowers, drifting clouds, and a moving sun. Combining generative patterns and nature-inspired elements, it creates a whimsical garden that evolves continuously with time.\n\n\n\n\nConcept Note"
  },
  {
    "objectID": "posts/welcome/index.html#a2.",
    "href": "posts/welcome/index.html#a2.",
    "title": "InkaOkati",
    "section": "",
    "text": "MudraNet\n\n\nAadil Tanwir, Naman Rajoria, Rayna Singh, Soumya Saboo, Sriharshitha Bhagvati\n:::: &lt;/p&gt;\n\n:::\nDownload Train ZIP file\nDownload Test ZIP file\nOur a2 explores how machine learning can recognize five classical mudras using hand gesture detection, and play a specific song for each one.\nProcess\nWe started by collecting images for each mudra. It was honestly tricky because some friends found it hard to pose like, they literally couldn’t bend their fingers that way.\nWhile researching how to train a model using images, we first came across TensorFlow. Then we found ml5.js was built on top of it. Arvind told us TensorFlow might be too complicated for us right now, so we went ahead with ml5.js.\nBefore jumping into handpose, we also tried out Feature Extractor with MobileNet. Feature Extractor uses MobileNet, which is a pre-trained ML model. After some trial and error and getting lost a bit, we sat down and made a proper step-by-step list to get clarity:\n\nFinish clicking and labeling all the images using our naming format.\nUse Python to generate a CSV with image paths and labels.\nUse a JS sketch to extract 21 keypoints for each image and generate a new CSV.\nTrain the model using this second CSV.\nTest the model with webcam input.\nAdd a song trigger for each mudra.\n\nHandpose detection on a single image worked perfectly we were able to extract 21 points and got excited that we could download a CSV and it was actually working.\nCreating the first CSV using Python was pretty simple.\nThe challenge started when we wrote a JS sketch to read that CSV and output the 21 keypoints for all the images. Initially, it was only giving us values for one image, or just the headings (x1, y1… x21, y21, label) but no actual values. We figured this was because the function to read all rows was in preload(), which runs too early the model takes time to train and wasn’t ready yet, so it only had time to process the first row. Once we moved that part to setup(), everything worked fine and we got the full CSV with coordinates. Another small mistake was that we were usign an older ml5js verson and that was causing a few errors.\nTraining\nA few images just wouldn’t get detected, especially ones where the hands had paint or black charcoal. We had to delete those.\nAll our training and testing so far was with our own dataset and images. So the next step was to combine other teams’ CSVs and train the model together with all of them, then link the mudras to songs.\nWhen we trained just using our team’s images, everything worked fine. But once we started combining datasets, label issues came up.\nFor example, “Arala” vs “arala” were treated as different classes, same with “katakamukha” vs “kataka-mukha”. While combining, we also saw that the pixel value ranges were totally different. One group had values ranging from 500 to 1000, while ours was more like 160 to 300. That’s when we realized we’d need to normalize the values before training.\nTrying varying epochs: 100 to 150 was enough for the samples that we have. We tried to understand the loss graph and we used it like a guide to find the sweet spot between the underfitting and overfitting. Epochs and batch size - It happens automatically if we don’t specify, and we didn’t do that because ml5 internally passes control to TensorFlow.js which sets a default batch size of 32.\n\nOnce we sorted all of that, the last step was adding the songs."
  }
]